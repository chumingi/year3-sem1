# 12주차 - 교재 내용 리뷰

## 3장 - 머신러닝의 기초

### 전통적인 프로그래밍 vs 인공지능 프로그래밍

- 110p 그림 참고
1. 전통적인프로그래밍
    - 사용자가 직접 규칙 정의
    - 프로그램을 개발하여 컴퓨터로 입력 데이터를 처리하 ⇒ 출력 데이터 생성
2. 인공지능 프로그래밍
    - 데이터에서 스스로 규칙 발견
    - 입력 데이터와 출력 데이터를 컴퓨터에 제공 ⇒ 컴퓨터가 스스로 프로그램 코드 생성

### 지도 vs 비지도 vs 강화

1. 지도학습
    - 주어진 에제(샘플)와 정답(레이블) 제공 ⇒ 일반적인 규칙, 패턴 학습
2. 비지도학습
    - 학습 알고리즘이 스스로 입력 데이터에서 패턴을 찾아 학습
3. 강화학습
    - 보상 및 처벌의 형태로 학습 데이터 제공
    - 강화: 환경에 맞게 적응하기 어려움

### 회귀 vs 분류

- 115p 그림 3-2
1. 회귀
    - 주어진 입력-출력 쌍 학습 ⇒ 새로운 입력이 들어왔을 때 합리적인 출력값 예측
    - 입력(x)과 출력(y)가 주어질 때 입력에서 출력으로의 매핑 함수를 학습하는 것
2. 분류
    - 입력을 2개 이상의 클래스로 나누는 것

### 머신 러닝 6단계

- 데이터 수집 → 전처리 → 모델 생성 → 훈련 → 테스트 → 평가

### 훈련데이터 vs 테스트데이터

- 훈련데이터: 모델의 학습에 사용하는 데이터
- 테스트데이터: 훈련된 모델의 성능을 평가하는 데 사용하는 데이터
- 모델의 일반화 성능을 좋게 하기 위해서 나누어서 사용

### 과적합

- 모델이 특정한 데이터에만 잘 맞는 문제
- 데이터가 10만 개 이상이어야 좋음. 100개 이하면 과적합 가능성

### 과소적합 해결방법

- 피처 엔지니어링: 기존의 특성들을 조합하여 새로운 특징 생성
- 데이터 증대: 데이터의 다양성이 부족할 때 사용 (잡음 추가 등) - 기존 데이터를 유지하면서 다양성을 증가시켜야 함 (과적합 위험성)

### 머신러닝 알고리즘 성능평가

1. 정확도
    - $정확도 = (올바르게 분류한 샘플) / (전체 샘플 수)$
    - 하나의 클래스가 다른 클래스에 비해서 항상 월등하게 많은 경우 문제 발생
2. 혼동행렬
    - 학습된 머신러닝 시스템이 예측을 하면서 얼마나 혼동하고 있는지를 나타내는 행렬
    - TP: 긍정을 긍정으로 정확히 예측
    - FN: 긍정을 부정으로 잘못 예측
    - FP: 부정을 긍정으로 잘못 예측
    - TN: 부정을 부정으로 정확히 예측
3. f1-score
    - 데이터의 불균형이 일어난 상황에서는 정확도만으로는 정확히 평가할 수 없음
    - precesion과 recall의 조합 평균
4. **Macro Average**
    - 각 클래스별 F1-score의 **단순 평균**
    - 모든 클래스에 동일한 중요도를 부여함
5. **Weighted Average**
- 각 클래스별 F1-score에 해당 클래스의 **샘플 수를 가중치로 반영**한 평균
- 전체 데이터에서 차지하는 비율에 따라 클래스별 평가 지표를 가중합

---

## 4장 - 선형회귀

### 학습과 손실 - 148~149p

- 학습: 머신런이에서 모델을 학습시키는 것 = 훈련 데이터로부터 손실을 최소화하는 가중치(w)와 바이어스(b)를 학습(결정)하는 것
- 손실: 평균과 실제값 사이의 차이. 예측이 완별하면 0, 그렇지 않으면 0보다 커짐

### 경사하강법 - 150p

- 손실을 최소화하기 위한 알고리즘
- 경사하강법을 구현한 모델이 옵티마이저
- 한 지점에서 출발해 최적값에 닿길 바라며 점점 이동해가는 방법
- $새로운 w = 이전 w - 학습률*미분값$
- w와 b 업데이트 - 153p
    - w = w - 0.01 * 기울기

---

## 5장 - 퍼센트론

### 퍼센트론

- 단 하나의 뉴런 사용
- **MLP**(멀티 레이어 퍼센트론) - 예) Dense

### 활성화함수 sigmoid

- 입력 값을 받아들일 것인가 아닌가
- 173p 표 5-2, 187p 표 계산 (176p 활성화함수 참고)
- $(w_1, 2_2, b)$가 주어질 때 각각의 $x_1$, $x_2$에 대해 $x_1w_1 + x_2w_2 + b$ 계산 ⇒ 활성화함수 적용하여 y 구하기
- **forward와 backward** 시험 출제

### 192p 연습문제 5-b

- 그림 5-14 표 수정 - -1.5 +0.5 +1.0 -1.0
- x=0, y=0일 때
    - $y_1 = x_1*w_1 + w_2*w_2 + b_1 = 0*(-1.0) + 0*1.0 + 0.5 = 0.5$, 양수이므로 1
    - $y_2 = x_1*w_2 + x_2*w_2 + b_2 = 0*(-1.0) + 0*1.0 + (-1.5) = -1.5$, 음수이므로 0
    - $y = 1*1.0 + 0*1.0 + (-1.5) = -0.5$, 음수이므로 0
- x=1, y=2일 때
    - y1 =

---

## 6장 - MLP(멀티 레이어 퍼센트론)

### 활성화함수 사용 이유

- 그림 196p 참고
- 선형 레이어을ㄹ 여러 개 쌓아도 의미 없음, 중간에 비선형 추가하여 고급화된 특성 추출 가능

### 활성화함수 종류

1. 계단함수
    - 입력 신호의 총합이 0을 넘으면 1 출력, 그렇지 않으면 0 출력
2. 시그모이드
    - 함수가 부드럽게 연결되기 때문에 어디서든 미분 가능 ⇒ 경사하강법 사용 가능
3. Relu
    - x가 0보다 작으면 0 출력, 크면 x 그대로 출력

### 활성화함수 계산 예제

- 202~203p 계산
- $h_1 = x_1w_1 + x_2w_2 + b_1 = 0 + 0 + 0.1 = 0.1$, 시그모이드 적용 ⇒ $a_1 = 0.52..$
- $h_2 = x_1w_3 + x_2w_4 + b_2 = 0 + 0 + 0.1 = 0.1$, 시그모이드 적용 ⇒ $a_2 = 0.52..$
- z_y = w_5a_1 + w_6a_2 + b_3 = 0.5*0.52.. + 0.6*0.52.. + 0.3 = 0.89.., 시그모이드 적용 ⇒ $a_y = 0.70..$
- 203p 행렬계산은 하지 않음

### 손실함수 계산

- 208~209p 참고, 215p 계산
- 다중분류일 때 각 손실의 평균을 구함
- 211p 손실함수 최소화 argmin
- 미분을 하는 이유: 기울기는 현재 가중치에서 손실이 증가/감소하는 방향 알려줌 → 이동해야 할 방향을 알려줌 (양수=손실증가: 반대방향으로 이동, 음수=손실감소: 진행방향으로 더 이동)

### 역전파

- 순전파처럼 하나씩 계산 - 체인룰 적용
- 202p out 계산 결과 활용
- 224p~225p 계산, 243p 7번 문제
- **역전파 계싼은 시험 출제 x**

---

## 7장 - MSP와 케라스 라이브러리

### 온라인학습 vs 풀배치학습 vs 미니배치학습 - 246p

1. 확률적 경사하강법 (=온라인학습)
    - 하나의 샘플을 처리한 후에 바로 가중치 변경
2. 풀 배치 학습
    - 모든 샘플을 모두 처리한 후에 가중치 변경
3. 미니 배치 학습
    - 훈련 샘플을 작은 배치들로 분리시킨 후에, 하나의 배치가 끝날 때마다 학습 수행

### 학습율 - 257p

- 너무 높다면: 오버슈팅 발생, 불안해지면 발산 가능
- 너무 낮다면: 아주 느리게 학습 진행, 수렴되는 시간 늦어짐

### Tensorflow vs pytorch

- 

### 모델 생성 3가지

1. Senquential 모델 만들고 모델에 필요한 레이어 추가하기
    - `add()`를 사용하여 모델에 점진적으로 레이어 추가
2. 함수형 API 사용
    - 레이어와 레이어를 함수로 연결
    - 중간에서 은닉층의 값 추출 가능
3. Model 클래스 상속받기

### 하이퍼파라미터 튜닝 - 288p

- 하이퍼파라미터: 신경망의 학습률, 은닉층 개수, 유닛 개수 학습 전에 설정해주어야 하는 변수들

### 그리드 서칭 - 290p

- 각 하이퍼 파라미터에 대하여 가장 좋은 조합을 찾아주는 알고리즘
- 머신러닝, 딥러닝 모두 사용 가능

---

## 8장 - 심층 신경망

### 그레디언트 손실 문제 (**중요)**

- 네트워크 레이어가 길어질수록 (여러 층 쌓을 때)
- 역전파 함수 값이 사라지는 문제
- 업데이트가 제대로 되지 않고 손실되는 문제 발생
- 302p 그림 8-5 참고 - sigmoid 함수 수정하여 양의 무한대 (relu)

### softmax - 305p

- 다중 분류할 때 출력층에 사용
- 최종값을 1이 엄지 않도록 확률적으로 변경해줌

### 손실함수 4가지 - 310p

1. binaryCrossentropy
    - 이진 분류 문제를 해결하는 데 사용되는 손실함수
    - 예: 이미지를 ‘강아지’와 ‘강아지 아님’의 두 분류로 나눔
    - 예측값이 실제 레이블에서 얼마나 떨어져 있는지 측정
2. CategoricalCrossentropy
    - 다중 분류 문제에서 사용되는 손실함수
    - 정답 레이블은 원-핫 인코딩으로 제공
    - 예: 이미지를 ‘강아지(1, 0, 0)’, ‘고양이(0, 1, 0)’, ‘호랑이(0, 0, 1)’ 중 하나로 분류
3. SparseCategoricalCrossentropy
    - 정답 레이블이 원-핫 인코딩이 아닌 정수로 주어질 때 사용하는 손실함수
    - 예: 정답 레이블이 0(강아지), 1(고양이), 2(호랑이)로 제공
4. MeanSquaredError
    - 회귀 문제에서 예측 값과 실제 값의 차이의 평균 제곱 오차를 계싼하기 위해 사용하는 손실함수

### 과잉 적합 방지 - 311p

1. 조기종료
2. 가중치 규제 (L1, L2)
3. 데이터 증가 (언더피팅 방지)
4. 드로우아웃 (334p)
    - 학습 과정에서는 렌덤하게 몇 개의 노드를 제외
    - 테스트 때에는 풀어줌

### 앙상블 - 337p

- 여러 개의 모델 섞어 사용