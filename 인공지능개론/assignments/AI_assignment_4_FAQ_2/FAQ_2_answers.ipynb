{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5779ef63-2cbd-4f62-9574-ea819c7484b3",
   "metadata": {},
   "source": [
    "# FAQ 2 답변들\n",
    "\n",
    "## 1. Activation function 함수를 사용하는 이유? Softmax, Sigmoid 함수의 차이는?\n",
    "\n",
    "### 1-1. Activation function 함수를 사용하는 이유?\n",
    "\n",
    "- 누가: 인공신경망을 학습시키는 개발자나 인공지능 알고리즘.\n",
    "- 언제: 각 뉴런의 입력값을 처리하고 출력값을 계산할 때.\n",
    "- 어디서: 인공신경망의 각 노드(뉴런)에서 사용됨.\n",
    "- 무엇을: 신경망에서 활성화 함수를 사용함.\n",
    "- 어떻게: 입력값에 비선형 함수를 적용하여, 출력값을 결정함.\n",
    "- 왜: 왜? 단순한 선형 함수로는 복잡한 데이터의 패턴을 제대로 학습할 수 없기 때문에\n",
    "\n",
    "인공신경망을 설계하고 학습시키는 개발자나 알고리즘은, 각 뉴런이 입력값을 처리하고 출력값을 계산할 때 활성화 함수를 사용한다. 단순한 선형 함수만으로는 복잡한 데이터의 패턴을 제대로 학습할 수 없기 때문에, 신경망의 각 노드(뉴런)에서 입력값에 비선형 함수를 적용해 출력값을 결정한다.\n",
    "\n",
    "### 1-2. Softmax, Sigmoid 함수의 차이는?\n",
    "\n",
    "#### 1-2-1. Softmax 함수\n",
    "\n",
    "- 누가: 다중 클래스 분류 문제를 해결하려는 모델이 사용함.\n",
    "- 언제: 마지막 출력층에서 다중 클래스 분류 시 사용.\n",
    "- 어디서: 분류 모델의 출력층.\n",
    "- 무엇을: 입력 벡터를 전체 합이 1이 되도록 정리해, 각 값이 차지하는 비중을 보여주는 정규화된 확률 분포로 바꿔줌.\n",
    "- 어떻게: 각 클래스의 지수 함수를 계산한 후, 전체 합으로 나눔.\n",
    "- 왜: 모든 클래스의 확률 합이 1이 되도록 하여, 가장 가능성 높은 클래스를 선택하기 위해.\n",
    "\n",
    "Softmax 함수는 여러 개의 클래스 중 하나를 선택해야 하는 다중 클래스 분류 문제에서 사용된다. 주로 모델의 마지막 출력층에서 적용되며, 입력 벡터의 각 값을 지수 함수로 바꾼 뒤 전체 합으로 나누는 방식으로 계산된다. 이렇게 하면 전체 값의 합이 1이 되도록 정규화된 확률 분포가 만들어지고, 이를 통해 가장 가능성 높은 클래스를 선택할 수 있다.\n",
    "\n",
    "#### 1-2-2. Sigmoid 함수\n",
    "\n",
    "- 누가: 이진 분류 모델 또는 은닉층 뉴런에서 사용함.\n",
    "- 언제: 출력층이 1개인 경우나 확률 출력이 필요한 중간 단계.\n",
    "- 어디서: 출력층 또는 은닉층의 뉴런에서 사용.\n",
    "- 무엇을: 입력값을 0과 1 사이의 실수값으로 변환.\n",
    "- 어떻게: $1 / (1 + e^(-x))$ 공식을 이용해 0~1 범위로 변환.\n",
    "- 왜: 어떤 결과가 나올 가능성이 얼마나 되는지를 알기 위해서.\n",
    "\n",
    "Sigmoid 함수는 이진 분류 문제에서 주로 사용되며, 출력층이 하나인 경우나 은닉층의 뉴런에서도 활용된다. sigmoid 함수는 $1 / (1 + e^{-x})$ 공식을 통해입력값을 0과 1 사이의 실수값으로 변환해 주다. 해당 입력이 '참'일 가능성이 얼마나 되는지를 수치로 표현할 수 있기 때문에 이진 판단이 필요한 상황에서 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b831b48-8a21-40c4-9bef-4664d67e5550",
   "metadata": {},
   "source": [
    "## 2. Forward propagation, Backward propagation이란?\n",
    "\n",
    "### 2-1. Forward Propagation (순전파)\n",
    "\n",
    "- 누가: 신경망의 각 층어)이.\n",
    "- 언제: 입력 데이터를 받아 예측값을 계산할 때.\n",
    "- 어디서: 신경망의 입력층부터 출력층까지 순차적으로.\n",
    "- 무엇을: 입력값을 기반으로 가중치와 편향을 적용해 예측값을 계산함.\n",
    "- 어떻게: 각 층의 가중합을 활성화 함수에 통과시켜 출력값을 전달함.\n",
    "- 왜: 입력에 대한 예측 결과를 도출하기 위해.\n",
    "\n",
    "Forward propagation는 신경망에서 입력층부터 출력층까지 데이터를 차례대로 전달하며 계산을 수행하는 과정이다. 입력값이 들어오면, 각 층의 뉴런들이 가중치와 편향을 적용한 후, 활성화 함수를 통해 변환된 값을 다음 층으로 전달하는 과정을 반복면서, 최종적으로 **예측 결과(출력값)**가 계산된다. 출력값은 학습에서 오차를 계산하고 가중치를 조정하는 기준이 된다.\n",
    "\n",
    "### 2-2. Backward Propagation (역전파)\n",
    "\n",
    "- 누가: 신경망 학습 알고리즘(예: 경사하강법)이.\n",
    "- 언제: 예측값과 실제값의 오차(손실)를 계산한 후.\n",
    "- 어디서: 출력층에서 시작해 입력층 방향으로 거꾸로.\n",
    "- 무엇을: 오차를 각 층의 가중치에 대해 미분한 기울기.\n",
    "- 어떻게: Chain Rule을 이용해 가중치별 기울기를 계산하고 업데이트.\n",
    "- 왜: 오차를 줄이기 위해 가중치와 편향을 수정하려고.\n",
    "\n",
    "Backward Propagation은 신경망 학습에서 예측값과 실제값 사이의 오차를 줄이기 위한 과정이다. 신경망 학습 알고리즘(경사하강법)은 출력값과 실제값 사이의 오차(손실)를 계산한 후에 이 오차를 줄이기 위해 출력층부터 입력층 방향으로 거꾸로 계산을 진행합니다. 연쇄법칙(Chain Rule)을 이용해 오차가 각 층의 가중치에 얼마나 영향을 미치는지를 나타내는 기울기(미분값)를 구한 후에 기울기를 바탕으로 가중치와 편향을 조정하면서 신경망이 더 정확한 예측을 할 수 있도록 학습을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61042224-bd95-4f7f-860a-3e041434f92b",
   "metadata": {},
   "source": [
    "## 3. 손실함수란 무엇인가? 가장 많이 사용하는 손실함수 4가지 종류는?\n",
    "\n",
    "### 3-1. 손실함수란 무엇인가?\n",
    "\n",
    "- 누가: 신경망 모델의 학습 알고리즘이.\n",
    "- 언제: 예측값과 실제값을 비교할 때.\n",
    "- 어디서: 모델의 출력층과 정답(label) 비교 과정에서.\n",
    "- 무엇을: 예측값과 실제값의 차이를 수치로 나타낸 함수.\n",
    "- 어떻게: 예측값과 실제값 사이의 오차를 계산하여 하나의 값으로 반환.\n",
    "- 왜: 학습이 제대로 되었는지 확인하고, 가중치와 편향을 업데이트하기 위해.\n",
    "\n",
    "손실 함수는 신경망 모델이 얼마나 정확하게 예측했는지를 수치로 평가해주는 함수이다. 모델이 예측을 마친 뒤, 출력층에서 실제 정답(label)과 비교하는 과정에서 학습 알고리즘이 손실 함수를 사용해 예측값과 실제값의 차이를 계산한다.이 함수는 예측값과 실제값 사이의 오차를 하나의 수치로 나타내며, 신경망이 학습을 제대로 하고 있는지를 확인하고, 가중치와 편향을 어떻게 조정해야 할지 결정하는 기준이 된다.\n",
    "\n",
    "### 3-2. 가장 많이 사용하는 손실함수 4가지 종류는?\n",
    "\n",
    "#### 3-2-1. Mean Squared Error (MSE, 평균 제곱 오차)\n",
    "\n",
    "- 누가: 회귀 문제를 해결하는 모델이.\n",
    "- 언제: 예측값과 실제값이 연속적인 수치일 때.\n",
    "- 어디서: 출력층과 정답값을 비교하는 단계에서.\n",
    "- 무엇을: 예측값과 실제값의 차이의 제곱 평균.\n",
    "- 어떻게: $(1/n) \\sum (y - \\hat{y})^2$ 방식으로 계산.\n",
    "- 왜: 큰 오차에 더 큰 패널티를 주어 안정적인 회귀 학습을 위해.\n",
    "\n",
    "Mean Squared Error는 회귀 문제를 해결하는 모델이 사용하며, 예측값과 실제값이 연속적인 수치일 때 활용된다. 출력층과 정답값을 비교하는 단계에서, 예측값과 실제값의 차이를 제곱해 평균을 내는 방식으로 계산하며, 큰 오차에 더 큰 패널티를 부여해 학습이 안정적으로 이루어지도록 한다.\n",
    "\n",
    "#### 3-2-2. Binary Cross Entropy (이진 교차 엔트로피)\n",
    "\n",
    "- 누가: 이진 분류를 수행하는 모델이.\n",
    "- 언제: 출력값이 0 또는 1 사이 확률일 때.\n",
    "- 어디서: 출력층에서 Sigmoid 함수를 사용한 뒤.\n",
    "- 무엇을: 실제값과 예측 확률 사이의 차이를 로그 기반으로 계산.\n",
    "- 어떻게: $-y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$.\n",
    "- 왜: 예측이 얼마나 정확한지를 확률을 기준으로 더 세밀하게 평가할 수 있기 때문.\n",
    "\n",
    "Binary Cross Entropy는 이진 분류를 수행하는 모델이, 출력값이 0과 1 사이의 확률로 표현될 때 사용된다. 보통 출력층에 Sigmoid 함수를 적용한 뒤, 예측 확률과 실제값의 차이를 로그 기반으로 계산하며, 예측이 얼마나 정확한지를 더 세밀하게 평가할 수 있도록 돕는다.\n",
    "\n",
    "#### 3-2-3. Categorical Cross Entropy (다중 클래스 교차 엔트로피)\n",
    "\n",
    "- 누가: 다중 클래스 분류 모델이.\n",
    "- 언제: 여러 클래스 중 하나를 선택할 때.\n",
    "- 어디서: 출력층에서 Softmax 함수를 적용한 뒤.\n",
    "- 무엇을: 실제 클래스와 예측 확률 분포 간의 차이를 계산.\n",
    "- 어떻게: $-\\sum y_i \\log(\\hat{y}_i)$.\n",
    "- 왜: 모델이 올바른 클래스를 선택했는지를 정확하게 측정하기 위해.\n",
    "\n",
    "Categorical Cross Entropy는 다중 클래스 분류 모델이, 여러 클래스 중 하나를 선택해야 할 때 사용한다. 출력층에서 Softmax 함수를 적용한 후, 예측된 확률 분포와 실제 클래스 간의 차이를 계산하고, 이를 통해 모델이 올바른 클래스를 선택했는지를 정밀하게 평가한다.\n",
    "\n",
    "#### 3-2-4. Sparse Categorical Cross Entropy \n",
    "\n",
    "- 누가: 다중 클래스 분류 문제를 푸는 신경망 모델의 학습 알고리즘이 사용함\n",
    "- 언제: 정답(label)이 정수 인덱스 형식(예: 0, 1, 2, ...)일 때, 예측과 실제값 사이의 오차를 계산할 때 사용\n",
    "- 어디서: 출력층에 Softmax 함수를 사용한 분류 모델의 손실 계산 단계에서\n",
    "- 무엇을: 예측된 확률 분포와 정답 클래스의 인덱스를 비교하여 오차를 계산하는 손실 함수\n",
    "- 어떻게: $−\\log(\\hat{𝑦})$\n",
    "- 왜: one-hot 인코딩 없이도 다중 클래스 분류를 간단하고 효율적으로 학습할 수 있도록 하기 위해\n",
    "\n",
    "Sparse Categorical Cross Entropy는 다중 클래스 분류 문제를 푸는 신경망 모델이, 정답이 one-hot 벡터가 아닌 정수 인덱스일 때 사용된다. Softmax 출력과 정답 인덱스를 직접 비교해 손실을 계산하며, one-hot 인코딩 없이도 다중 클래스 분류를 효율적으로 처리할 수 있도록 설계되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f35e78-9dbf-4715-b057-ef30cdec0fe3",
   "metadata": {},
   "source": [
    "## 4. 옵티마이저(optimizer)란 무엇일까? 옵티마이저와 손실함수의 차이점은?\n",
    "\n",
    "### 4-1. 옵티마이저(Optimizer)란 무엇일까?\n",
    "\n",
    "- 누가: 인공신경망 학습 알고리즘이.\n",
    "- 언제: 손실함수를 최소화하기 위해 가중치를 업데이트할 때.\n",
    "- 어디서: 학습 과정 전반에서 반복적으로 적용됨.\n",
    "- 무엇을: 손실값을 줄이기 위해 가중치와 편향을 조정하는 방법 또는 알고리즘.\n",
    "- 어떻게: 손실 함수의 기울기(gradient)를 계산하고, 이를 기반으로 파라미터를 업데이트함.\n",
    "- 왜: 모델의 예측 성능을 향상시키기 위해.\n",
    "\n",
    "옵티마이저는 인공신경망이 학습 중 손실함수를 최소화할 수 있도록, 모델의 가중치와 편향을 반복적으로 조정하는 알고리즘이다. 신경망은 출력값과 실제값의 차이를 손실함수로 계산한 뒤, 그 손실을 줄이기 위해 각 가중치에 대한 기울기를 계산하고 매 반복마다 값을 조정한다. 옵티마이저는 가중치 조정을 어떻게, 얼마나 조정할지 결정한다.\n",
    "\n",
    "### 4-2. 옵티마이저와 손실함수의 차이점은?\n",
    "\n",
    "손실함수는 모델의 예측이 얼마나 틀렸는지 수치화하여 알려주는 함수로, 학습의 기준이 된다. 옵티마이저는 그 손실값을 최소화하는 방향으로 가중치와 편향을 조정**해나가는 역할을 한다. 손실함수는 “얼마나 틀렸는가”를 알려주고, 옵티마이저는 “틀린 것을 어떻게 고칠 것인가”를 수행한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a9fd9-715c-4191-8d1b-79e5bb57b1b6",
   "metadata": {},
   "source": [
    "## 5. 경사하강법 의미는?\n",
    "\n",
    "### 5-1. 경사하강법(Gradient Descent)란?\n",
    "\n",
    "- 누가: 인공신경망 학습 알고리즘이.\n",
    "- 언제: 손실 함수 값을 최소화하려고 할 때.\n",
    "- 어디서: 모든 가중치와 편향을 업데이트하는 학습 반복 과정에서.\n",
    "- 무엇을: 손실 함수의 기울기를 따라 가중치를 조정하는 최적화 방법.\n",
    "- 어떻게: 각 가중치에 대한 손실 함수의 미분값(기울기)을 계산해, 손실이 줄어드는 방향으로 파라미터를 업데이트함.\n",
    "- 왜: 모델의 성능을 개선하기 위해 손실값을 최소화하려고.\n",
    "\n",
    "경사하강법은 인공신경망이 손실 함수의 값을 최소화하기 위해 사용하는 최적화 알고리즘이다. 손실 함수의 기울기를 계산하여, 가장 빠르게 손실이 줄어드는 방향(음의 기울기 방향)으로 가중치와 편향을 조금씩 이동시킨다. 이 과정을 반복면 손실 함수의 값은 줄어들고 모델의 예측 정확도는 높아진다.\n",
    "\n",
    "### 5-2. 경사하강법의 종류: 확률적 / 배치 / 미니배치\n",
    "\n",
    "#### 5-2-1. 배치 경사하강법 (Batch Gradient Descent)\n",
    "\n",
    "- 누가: 전체 학습 데이터를 한꺼번에 사용하는 학습 알고리즘이.\n",
    "- 언제: 각 반복(epoch)마다 전체 데이터를 기반으로 가중치를 업데이트할 때.\n",
    "- 어디서: 학습 데이터셋 전체를 한 번에 계산 가능한 환경에서.\n",
    "- 무엇을: 전체 데이터에 대한 평균 기울기를 계산하여 파라미터 업데이트.\n",
    "- 어떻게: 모든 샘플에 대해 손실을 계산하고 평균을 낸 후 파라미터 수정.\n",
    "- 왜: 안정적인 수렴과 정확한 방향성을 위해.\n",
    "\n",
    "배치 경사하강법은 전체 학습 데이터를 한꺼번에 사용하여 학습하는 알고리즘이다. 매 반복(epoch)마다 모든 데이터를 기반으로 손실을 계산한 뒤에 가중치와 편향을 업데이트한다. 주로 전체 데이터를 한 번에 계산할 수 있는 환경에서 사용되며, 학습이 더 안정적으로 수렴하고 보다 정확한 방향으로 업데이트가 가능하다.\n",
    "\n",
    "#### 5-2-2. 확률적 경사하강법 (Stochastic Gradient Descent, SGD)\n",
    "\n",
    "- 누가: 개별 샘플 단위로 학습하는 알고리즘이.\n",
    "- 언제: 학습 데이터의 한 샘플을 이용해 매 반복마다 업데이트할 때.\n",
    "- 어디서: 대규모 데이터나 실시간 처리 환경에서.\n",
    "- 무엇을: 샘플 하나에 대한 손실 함수의 기울기를 계산.\n",
    "- 어떻게: 하나의 데이터를 기준으로 손실을 계산하고 즉시 가중치 수정.\n",
    "- 왜: 빠른 업데이트와 적은 메모리 사용을 위해.\n",
    "\n",
    "확률적 경사하강법은 개별 데이터 샘플을 기준으로 학습을 진행하는 알고리즘이다. 매 반복마다 학습 데이터에서 하나의 샘플을 선택해 손실 함수를 계산하고, 그에 따라 가중치를 바로 업데이트한다. 확률적 경사하강법은 대규모 데이터나 실시간 처리 환경에서 효과적이며, 샘플 하나에 대한 기울기만 계산하기 때문에 메모리 사용량이 적고 빠르게 학습을 진행할 수 있다. \n",
    "\n",
    "#### 5-2-3. 미니배치 경사하강법 (Mini-batch Gradient Descent)\n",
    "\n",
    "- 누가: 소규모 묶음(mini-batch) 단위로 학습하는 알고리즘이.\n",
    "- 언제: 각 반복마다 일정 수의 샘플 묶음을 이용해 가중치를 업데이트할 때.\n",
    "- 어디서: 대부분의 딥러닝 프레임워크와 GPU 환경에서.\n",
    "- 무엇을: 미니배치에 대한 평균 손실을 바탕으로 파라미터를 업데이트.\n",
    "- 어떻게: 지정된 batch size만큼 데이터를 나눠 학습하고, 매 반복마다 그 묶음 단위로 업데이트.\n",
    "- 왜: 속도와 안정성의 균형을 맞추기 위해.\n",
    "\n",
    "미니배치 경사하강법은 데이터를 일정한 크기의 묶음(mini-batch)으로 나누어 학습하는 알고리즘이다. 매 반복마다 정해진 개수의 샘플을 사용해 손실을 계산하고, 해당 묶음의 평균 기울기를 바탕으로 가중치를 업데이트한다. 미니배치 경사하강법은 딥러닝 프레임워크와 GPU 환경에서 특히 효율적이며, 속도와 학습 안정성 사이의 균형을 잘 맞출 수 있다. 전통적인 배치 학습보다 빠르고, 확률적 방식보다 수렴이 안정적이기 때문에 실제 딥러닝 모델 학습에서 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5463251-e482-4942-a5b8-a84c2a385510",
   "metadata": {},
   "source": [
    "## 6. CNN의 합성곱의 역할은?\n",
    "\n",
    "- 누가: 합성곱 계층이.\n",
    "- 언제: 입력 이미지나 특성맵을 처리할 때.\n",
    "- 어디서: CNN의 앞부분에서 반복적으로 적용됨.\n",
    "- 무엇을: 필터를 이용해 이미지의 특징을 추출함.\n",
    "- 어떻게: 필터를 이미지 위에 슬라이딩하며 합성곱 연산을 수행.\n",
    "- 왜: 지역적인 패턴(모서리, 윤곽선, 질감 등)을 인식하기 위해.\n",
    "\n",
    "합성곱은 CNN에서 입력 이미지나 특성 맵을 처리할 때 핵심적인 역할을 한다. CNN의 앞부분에서 반복적으로 적용되며, 작은 크기의 필터(커널)를 이미지 위에 슬라이딩하면서 합성곱 연산을 반복한다. 합성곱 연산을 통해 모델은 모서리, 윤곽선, 질감 등과 같은 지역적인 패턴을 인식하여 이미지의 주요 특징을 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fd5ae-94f3-43c0-91a2-416abfd247c4",
   "metadata": {},
   "source": [
    "## 7. CNN의 풀링층의 역할은?\n",
    "\n",
    "- 누가: 풀링 계층이.\n",
    "- 언제: 합성곱 연산 후 주요 특징만 남기고 불필요한 정보를 줄일 때.\n",
    "- 어디서: 합성곱 계층 뒤에 위치하여 반복 적용됨.\n",
    "- 무엇을: 특성맵의 공간 크기를 줄이고 중요한 정보만 유지함.\n",
    "- 어떻게: 일정 범위 내에서 최댓값(Max Pooling)이나 평균값(Avg Pooling)을 추출함.\n",
    "- 왜: 연산량을 줄이고 과적합을 방지하며 중요한 특징만 추출하기 위해.\n",
    "\n",
    "풀링 계층은 합성곱 연산 이후에 위치하여 반복적으로 적용되며, 특성맵에서 중요한 정보만 남기고 불필요한 데이터를 줄이는 역할을 한다. 일정 범위 내에서 최댓값이나 평균값을 추출하는 방식으로 공간 크기를 줄이고 중요 정보만 유지한다. 연산량을 감소시키고 과적합을 방지하며 모델이 더 적은 정보로도 핵심적인 특징을 효율적으로 학습할 수 있게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d73bb-7a76-4a38-81a1-bea72f07b0aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. CNN의 Dense Layer의 역할은?\n",
    "\n",
    "- 누가: 완전연결층(Dense Layer)이.\n",
    "- 언제: 마지막 합성곱/풀링이 끝난 후 분류나 회귀 결과를 계산할 때.\n",
    "- 어디서: CNN 구조의 가장 마지막 부분에 위치.\n",
    "- 무엇을: 앞서 추출된 특징을 기반으로 최종 예측을 수행함.\n",
    "- 어떻게: 모든 뉴런이 서로 연결된 구조에서 가중치 계산을 통해 출력값 생성.\n",
    "- 왜: 추출된 특징을 바탕으로 특정 클래스나 값을 예측하기 위해.\n",
    "\n",
    "Dense Layer는 CNN 구조의 가장 마지막에 위치하여, 합성곱과 풀링 계층에서 추출된 특징들을 바탕으로 최종 예측을 수행한다. 모든 뉴런이 서로 연결된 구조에서 가중치 계산을 통해 하나의 출력값이 생성한다. 이 과정을 통해 모델은 입력 이미지가 어떤 클래스에 속하는지 또는 어떤 값을 가지는지를 예측하기 위해 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e27403-36f2-4460-9571-b0c6a660550a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9. CNN의 stride, filter의 역할? 필터의 가중치는 어떻게 결정되는가?\n",
    "\n",
    "### 9-1. CNN의 stride의 역할은?\n",
    "\n",
    "- 누가: 합성곱 연산을 수행하는 필터가.\n",
    "- 언제: 입력 이미지나 특성맵에 대해 이동하며 연산할 때.\n",
    "- 어디서: CNN의 합성곱 계층에서 적용됨.\n",
    "- 무엇을: 필터가 한 번에 얼마나 많이 이동할지를 정하는 값.\n",
    "- 어떻게: 한 번에 한 칸(stride=1) 또는 그 이상(stride=2 등)씩 수평·수직으로 이동함.\n",
    "- 왜: 출력의 크기를 조절하고 연산 속도 및 특성 추출 범위를 조정하기 위해.\n",
    "\n",
    "Stride는 합성곱 계층에서 필터가 입력 이미지나 특성맵 위를 이동할 때 한 번에 얼마나 많이 움직일지를 결정하는 값이다. 필터가 수평과 수직 방향으로 이동할 때, stride 값이 1이면 한 칸씩, 2 이상이면 그만큼 건너뛰며 이동한다. stride는 출력 특성맵의 크기를 조절하고, 연산 속도와 자원 사용량을 조정하기 위해 사용한다.\n",
    "\n",
    "### 9-2. CNN의 filter의 역할은?\n",
    "\n",
    "- 누가: CNN의 합성곱 계층이.\n",
    "- 언제: 입력 데이터(이미지 등)에서 특징을 추출할 때.\n",
    "- 어디서: 입력과 출력 사이에서 직접적으로 적용됨.\n",
    "- 무엇을: 이미지의 국소적인 특징을 추출함.\n",
    "- 어떻게: 필터가 입력의 특정 영역에 대해 가중합 연산을 수행함.\n",
    "- 왜: 모서리, 질감, 윤곽 등 유용한 시각적 특징을 감지하기 위해.\n",
    "\n",
    "CNN의 합성곱 계층은 입력 이미지에서 특징을 추출할 때 필터를 사용한다. 입력과 출력 사이에 직접적으로 작용하며, 특정 영역에 대해 입력값에 가중치를 곱하고 더하는 연산을 반복한다. 필터는 학습을 통해 조정되는 고유한 가중치 집합으로, CNN이 복잡한 구조를 갖는 이미지에서도 모서리, 윤곽, 질감 등 핵심적인 정보를 효율적으로 감지할 수 있게 해준다.\n",
    "\n",
    "### 9-3. 필터의 가중치는 어떻게 결정되는가?\n",
    "\n",
    "- 누가: 역전파와 옵티마이저가.\n",
    "- 언제: 모델이 손실함수를 계산하고 파라미터를 갱신할 때.\n",
    "- 어디서: 학습 과정 중 CNN의 각 필터에 대해.\n",
    "- 무엇을: 손실값을 줄이는 방향으로 필터의 가중치를 조정함.\n",
    "- 어떻게: 손실 함수의 기울기를 계산하고 경사하강법을 통해 업데이트함.\n",
    "- 왜: 입력에 대한 예측을 더 정확히 하기 위해.\n",
    "\n",
    "CNN에서 필터의 가중치는 학습 과정 중 자동으로 조정된다. 모델이 예측을 수행한 후 손실함수를 계산하고, 이 손실값을 줄이기 위해 각 필터의 가중치를 조정한다. 이 과정은 역전파와 옵티마이저가 손실 함수의 기울기를 계산하고 경사하강법을 이용해 파라미터를 업데이트하는 방식으로 이루어지며, 입력에 대한 예측을 더욱 정확하게 만들기 위해 사용된-다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb64af1-d997-4725-a922-ef3a623c5a21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10. RNN을 사용하는 이유와 한계점은?\n",
    "\n",
    "### 10-1. RNN을 사용하는 이유는?\n",
    "\n",
    "- 누가: 시계열, 자연어, 음성 등 순서가 중요한 데이터를 다루는 모델이.\n",
    "- 언제: 입력 데이터가 시간 또는 순서의 흐름을 갖고 있을 때.\n",
    "- 어디서: 자연어 처리, 음성 인식, 주가 예측 등 순차적 정보가 중요한 곳에서.\n",
    "- 무엇을: 이전의 정보를 기억하여 현재 입력과 결합해 처리함.\n",
    "- 어떻게: 은닉 상태(hidden state)를 순차적으로 전달하며 과거 정보를 반영함.\n",
    "- 왜: 단순한 신경망으로는 시간 의존성이 있는 정보를 처리할 수 없기 때문.\n",
    "\n",
    "RNN은 시계열, 자연어, 음성처럼 순서가 중요한 데이터를 처리하는 데 사용된다. 입력 데이터에 시간적 흐름이나 순차적 구조가 있을 때, RNN은 은닉 상태를 순차적으로 전달하여 이전 정보를 기억하고 현재 입력과 결합하여 처리한다. 일반적인 신경망으로는 시간 의존성이 있는 정보를 효과적으로 다룰 수 없기 때문에 자연어 처리, 음성 인식, 주가 예측 등 순차적 특성이 중요한 다양한 분야에서 RNN이 활용되며, \n",
    "\n",
    "### 10-2. RNN의 한계점은?\n",
    "\n",
    "- 누가: 기본 구조의 RNN이.\n",
    "- 언제: 긴 시퀀스를 학습하려고 할 때.\n",
    "- 어디서: 긴 문장이나 장기적인 시간 관계가 있는 데이터에서.\n",
    "- 무엇을: 중요한 정보를 오래 기억하거나 멀리 있는 정보와 연결하는 데 어려움을 겪음.\n",
    "- 어떻게: 역전파 과정 중 기울기가 너무 작아지는 문제(기울기 소실) 발생.\n",
    "- 왜: 반복 계산 구조에서 미분값이 반복적으로 곱해지면서 값이 사라지거나 폭주하기 때문.\n",
    "\n",
    "기본 구조의 RNN은 긴 문장이나 장기적인 시간 관계가 있는 데이터에서, 중요한 정보를 오래 기억하거나 멀리 떨어진 정보와 연결하는 데 한계까 있다. 역전파 과정 중 기울기가 반복적으로 곱해지면서 점점 작아지는 기울기 소실 문제가 발생하기 때문에 장기 의존성을 가진 데이터를 처리하는 데 제약이 생긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8518f3ab-d1a7-4cc5-9c07-11cde906643f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 11. LSTM을 사용하는 이유와 한계점은?\n",
    "\n",
    "### 11-1. LSTM을 사용하는 이유는?\n",
    "\n",
    "- 누가: 장기 의존성 정보를 처리하려는 시퀀스 기반 딥러닝 모델이.\n",
    "- 언제: 입력 데이터 내의 정보가 긴 시간 간격을 두고 관련되어 있을 때.\n",
    "- 어디서: 자연어 처리(NLP), 음성 인식, 시계열 예측 등 다양한 분야에서.\n",
    "- 무엇을: RNN의 한계였던 장기 의존성 문제를 해결하는 구조로 사용함.\n",
    "- 어떻게: 셀 상태와 3개의 게이트(입력, 삭제, 출력 게이트)를 통해 정보 흐름을 조절함.\n",
    "- 왜: RNN은 과거 정보를 오래 유지하지 못하기 때문에, 중요한 정보를 오랫동안 기억하기 위해.\n",
    "\n",
    "LSTM은 장기 의존성을 가진 정보를 효과적으로 처리하기 위해 사용된다. 시계열 예측, 자연어 처리, 음성 인식 등 순서가 중요한 데이터에서, 입력 간의 관련성이 긴 시간 간격을 두고 나타날 경우 기존 RNN으로는 정보를 오래 유지하는 데 한계가 있다. RNN의 문제를 해결하기 위해 LSTM은 셀 상태를 중심으로 입력/삭제/출력 게이트를 통해 어떤 정보를 저장하고, 제거하고, 출력할지를 스스로 결정한다. LSTM은 RNN보다 훨씬 더 오랫동안 중요한 정보를 기억하고 활용할 수 있다.\n",
    "\n",
    "### 11-2. LSTM의 한계점은?\n",
    "\n",
    "- 누가: LSTM 네트워크가.\n",
    "- 언제: 복잡한 구조와 많은 파라미터를 처리할 때.\n",
    "- 어디서: 데이터가 많거나 실시간 처리가 요구되는 환경에서.\n",
    "- 무엇을: 높은 계산량과 메모리 사용량을 요구함.\n",
    "- 어떻게: 여러 개의 게이트와 상태를 관리하므로 연산량이 큼.\n",
    "- 왜: 정교한 정보 흐름을 조절하기 위해 구조가 복잡하게 설계되어 있기 때문.\n",
    "\n",
    "LSTM 네트워크는 정교한 정보 흐름을 조절하기 위해 구조가 복잡하게 설계되어 있기 때문에, 여러 게이트와 상태를 관리해야 한다. 연산량이 많고 파라미터 수도 늘어나며, 학습과 추론 과정에서 많은 계산량과 메모리 용량을 필요로 한다. 특히 데이터가 많거나 실시간 처리가 필요한 환경에서는 성능 저하나 처리 지연으로 이어질 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b26ad-f4b0-48a0-bb2d-b8e1cf17ea72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 12. GRU를 사용하는 이유와 차별성은?\n",
    "\n",
    "### 12-1. GRU를 사용하는 이유는?\n",
    "\n",
    "- 누가: 순차적 패턴을 학습하는 시계열 기반 모델이.\n",
    "- 언제: 장기 의존성과 계산 효율성을 동시에 고려해야 할 때.\n",
    "- 어디서: 자연어 처리, 음성 인식, 시계열 예측 등에서.\n",
    "- 무엇을: LSTM과 유사한 방식으로 정보를 기억하고 잊음.\n",
    "- 어떻게: 업데이트 게이트와 리셋 게이트를 이용해 정보 흐름을 조절함.\n",
    "- 왜: LSTM보다 간단한 구조로도 유사한 성능을 낼 수 있기 때문에.\n",
    "\n",
    "GRU는 시계열 기반 모델이 순차적인 패턴을 학습할 때, 장기 의존성과 계산 효율성을 동시에 고려해야 하는 상황에서 사용된다. 자연어 처리, 음성 인식, 시계열 예측 등 다양한 분야에서 활용되며, LSTM처럼 정보를 기억하거나 잊는 기능을 수행하지만 더 단순한 구조를 가진다. GRU는 업데이트 게이트와 리셋 게이트를 통해 정보의 흐름을 효과적으로 조절하며, 복잡한 LSTM 구조 없이도 유사한 수준의 성능을 낼 수 있어 연산 효율이 중요한 환경에서 많이 사용된다.\n",
    "\n",
    "### 12-2. GRU의 차별성은?\n",
    "\n",
    "- 누가: GRU와 LSTM이.\n",
    "- 언제: 장기 의존성과 계산 자원의 균형이 요구되는 상황에서 비교될 때.\n",
    "- 어디서: 동일한 문제에 대해 두 모델을 비교할 수 있는 실험 환경에서.\n",
    "- 무엇을: 내부 구조와 정보 제어 방식이 다름.\n",
    "- 어떻게: GRU는 2개의 게이트만 사용하고 셀 상태를 분리하지 않음.\n",
    "- 왜: 연산 효율성과 간단한 구현을 위한 설계 철학이 다르기 때문.\n",
    "\n",
    "GRU와 LSTM은 모두 장기 의존성을 처리할 수 있는 순환 신경망 구조이지만, 내부 구조와 정보 제어 방식에서 차이가 있다. 두 모델이 동일한 문제를 다룰 때, GRU는 계산 자원의 효율성과 간결한 구조를 가진다. 두 가지 게이트(업데이트/리셋)만 사용하고 별도의 셀 상태를 분리하지 않는다. LSTM은 입력, 삭제, 출력 게이트와 셀 상태를 따로 관리하여 정보를 정교하게 조절한다. GRU는 연산 효율성과 구현 간편성을, lSTM은 정밀한 정보 제어를 목표로 설계된 점이 두 모델의 차별점이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a128dc6-dbfb-4b3a-ad32-2f344fb3dafe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 13. Dense Layer란 무엇인가?\n",
    "\n",
    "- 누가: 딥러닝 모델의 뉴런들이.\n",
    "- 언제: 특징을 종합하거나 최종 예측값을 계산할 때.\n",
    "- 어디서: CNN, RNN, MLP 등의 중간층이나 출력층 등 다양한 위치에서.\n",
    "- 무엇을: 입력층의 모든 뉴런과 출력층의 모든 뉴런을 연결하여 정보를 처리함.\n",
    "- 어떻게: 각 입력값에 대해 고유한 가중치를 곱하고 편향을 더한 뒤, 활성화 함수를 적용하여 출력을 생성함.\n",
    "- 왜: 입력된 정보 간의 복잡한 관계를 학습하고, 이를 바탕으로 분류나 회귀 등의 최종 결과를 도출하기 위해.\n",
    "\n",
    "Dense Layer는 딥러닝 모델에서 뉴런들이 입력된 정보를 종합하거나 최종 예측값을 계산할 때 사용하는 층이다. Dense Layer는 CNN, RNN, MLP 등 다양한 모델의 중간층이나 출력층에서 사용되며, 입력층의 모든 뉴런을 출력층의 모든 뉴런과 완전히 연결하여 정보를 처리한다. 각 입력값에는 고유한 가중치가 곱해지고, 편향이 더해진 후 활성화 함수를 통해 최종 출력이 생성된다. 모델은 입력된 정보들 간의 복잡한 관계를 학습하고, 그 결과를 바탕으로 분류나 회귀와 같은 최종 판단을 내릴 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
