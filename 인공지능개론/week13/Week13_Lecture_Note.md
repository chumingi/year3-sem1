# 13주차

## 리뷰

### Dense와 CNN의 차이

- Dense: 전체 특성에 대한 가중치와 편향 계싼. 특성들 간의 관계가 잘 학습되지 못하는 문제
- CNN: 필터를 이용하여 국소적인 특징 추출

### Pretrained model - 404~407

- 깊고 많은 신경망에서, 앞부분 활용하여 가중치, 편향 구하기 (일부 레이어만 학습)
- 앞쪽(입력에 가까운  부분)은 일반적인 특징 추출
- 뒤쪽에서 구체적인 특징 추출

### 파인 튜닝

- 근사치에 도달, 나의 상황에 맞도록 미세 조정
- pretrained model에서의 freezing을 해제한 후에, 전체 데이터에 대해 가중치와 편향 구하기 - 시간 적게 소요

### 데이터 증대 (data argumentation)

- 언더피팅 방지 - 데이터가 너무 적기 때문에 데이터를 늘리기 위해 사용
- 증대방법: 밝기 조정, 회전, smoth

---

## RNN

### 순차데이터(시계열 데이터) - 416p

- 시간적, 공간적 순서가 있는 데이터
- 예: 주식가격, 텍스트데이터, 오디오데이터 등 (모든 데이터 간에 순서가 있음)
- 과거의 사건과 현재의 사건이 상호의존적인 관계가 있을 때, 과거의 사건을 반영해야 함
- DNN/CNN에서는 현재 처리하는 데이터가 이전/다음 데이터와 관계 없음

### 순환 신경망 계열

- 과거의 사건/데이터가 현재의 사건/데이터에 영향을 미침
- 현재의 입력과 과거의 상태를 함께 고려

### 순환 데이터의 이해

- 이전의 데이터를 얼마나 반영할 것인가? 윈도우의 크기 설정
- 순환 신경망을 학습시키려면: 데이터를 일정한 크기로 잘라서 여러 개의 훈련 샘플을 만들어야 함 (이 샘플들은 부분적으로 중첩됨)
- 샘플의 크기 = 윈도우 ⇒ 이전 3개 데이터의 패턴을 가지고 다음 데이터 예측
- split_sequence 파이썬 함수
    
    ```python
    def split_sequence(sequence, n_steps):
        X, y = [], []
        for i in range(len(sequence) - n_steps):
            # 입력 시퀀스와 다음 값 분리
            seq_x = sequence[i:i + n_steps]
            seq_y = sequence[i + n_steps]
            X.append(seq_x)
            y.append(seq_y)
        return np.array(X), np.array(y)
    ```
    
- 420p ~ 421p 주식 가격 예측 실습

## RNN 정리

- CNN: 현재 상태만 고려
- RNN: 현재의 입력과 과거의 상태를 모두 고려
- 은닉: 현재와 과거 중 어디에 가중치를 둘 것인가
- 단점: 히든 상태까지 고려해야 하기 때문에 시퀀스 증가하면 백프로파게이션에서 계산 복잡
    
    → 시간이 오래 걸리고, 그레디언트 손실 문제 발생
    

---

## LSTM (Long Short Term Memory)

- 그레디언트 손실 문제 해결 가능
- RNN + 장기억 관련된 부분(따로 기억)
- 게이트: 숏텀, 히든, 출력에 무엇을 보낼 것인지를 결정 (LSTM 안의 정보들은 게이트를 통해 추가/삭제)

### 게이트 4가지 - 436~437p

- LSTM 유닛은 저장게이트, 삭제게이트, 업데이트게이트, 출력게이트를 가짐
1. 저장게이트
    - 셀 상태에 관련되는 새로운 정보를 저장
    - 현재의 입력과 과거의 히든 상태를 셀에 반영할 것인지를 결정
2. 삭제 게이트
    - 예전 상태 중에서 일부 관련이 없는 정보를 지움
    - 현재의 입력 상태를 반영할 것인지를 판단하여 삭제 여부 결정
    - 0에 가까우면 정보 많이 삭제, 1에 가까우면 정보 많이 살아남음
3. 업데이트 게이트
    - 선택적으로 셀 상태 값을 업데이트
4. 출력 게이트
    - 어떤 정보를 다음 시점 셀로 출력할지 제어
    - 현재입력과 과거 히든상태를 고려하여 다음 히든상태 결정

### 단점

- forward propagation, backward propagation 모두 계산 복잡, 속도 오래 걸림 ⇒ 그레디언트 손실 문제 발생

### 그레디언트 폭증 - 432p

- 그래디언트가 너무 커지는 문제
- 그래디언트를 일정 크기 이상 커지지 못하게 하는 방법 사용

---

## 정리

- CNN: 현재의 입력에 대한 현재의 출력을 결정
- RNN: 과거의 상태도 함께 고려
- RNN - 그레디언트 손실 문제 발생 ⇒ LSTM
- LSTM: 장기기억을 관리하는 셀 추가
- 입력, 삭제, 업데이트, 출력 게이트